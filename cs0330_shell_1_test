#!/usr/bin/env python3
from subprocess import PIPE
from subprocess import Popen
from glob import glob
from getopt import gnu_getopt
from getopt import GetoptError
import os
from os import chdir
import time
import signal
import sys
import tempfile
from termcolor import colored
from typing import Optional
from io import TextIOWrapper

execsBin = "/course/cs0330/bin/33sh"
pt_harness = "cs0330_pt_harness"

progFile = "noprompt"
testInput = "input"
testOutput = "output"
testError = "error"
testPoints = "points"
testSetup = "setup"
rubric = "rubric.txt"

def test_dir_to_str(test_dir: str) -> str:
    """
    Converts a test directory to its printable string equivalent.
    Args:
        test_dir (str): the test directory name.
    Returns:
        str: a printable equivalent of the directory name.
    """
    tokens = test_dir.split("_")
    return "Test %s (%s)" % (tokens[0], " ".join(tokens[1:]))

def color_text(text: str, color: "Optional[str]" = None, attrs: "list[str]" = []):
    """
    Colors the given text according to the parameters, or if a report file
    was specified, simply returns the original text.
    Args:
        text (str): the string to color
        color (Optional[str]): the optional color of the text
        attrs (list[str]): any other formatting attributes of the text
    Returns:
        str: the (potentially) colored text
    """
    return text if report else colored(text, color, attrs=attrs)


# cleans out a directory
def cleanDir(path: str) -> None:
    """
    Removes all items and directories in a given directory, including the directory itself.
    Args:
        path (str): the path to the given directory.
    Returns:
        None
    """
    for d in glob(path + "/*"):
        if os.path.isdir(d):
            cleanDir(d)
            os.rmdir(d)
        else:
            os.remove(d)


# run bash setup script for a test in directory
def setupDir(path: str, tpath: str) -> None:
    """
    Sets up the given temporary directory for use by a given test, by running the bash 
    setup script for that test.
    Args:
        path (str): the path to the directory to be used by the given test.
        tpath (str): the path to the test's directory.
    Returns:
        None
    """        
    pid = os.fork()
    if pid == 0:
        try:
            os.execv(tpath + "/" + testSetup, [tpath + "/" + testSetup, playground + "/"])
        except OSError as e:
            print("Error executing setup script '%s'." % (tpath + "/" + testSetup))
            print(e)
        sys.exit(1)
    else:
        os.waitpid(pid, 0)


def runShell(shell_path: str, test_dir: str, shell_input: str, 
             report_file: Optional[TextIOWrapper]) -> Optional["tuple[str, str]"]:
    """
    Runs the given shell program on a given test, returning a tuple of the process's 
    output and error output on success, or None on failure.
    Args:
        shell_path (str): the absolute path to the shell executable.
        test_dir (str): the absolute path to the directory of the test being run.
        shell_input (str): the input to be fed to the shell executable, with each 
            command on a new line.
        report_file (Optional[TextIOWrapper]): the optional file to print a report to.
    Returns:
        Optional[tuple[str, str]]: either the shell program's output and error output on success,
            or None on failure.
    """
    # start shell
    prog = None
    try:
        if pseudoterm:
            prog = Popen([execsBin + "/" + pt_harness, shell_path], 
                         stdin=PIPE, stdout=PIPE, stderr=PIPE, text=True)
        else:
            prog = Popen([shell_path], stdin=PIPE, stdout=PIPE, stderr=PIPE, text=True)
    except (OSError, IOError) as e:
        print("Error executing shell '%s'." % (shell_path))
        print(e)
        sys.exit(1)

    prev_line = None
    for line in shell_input.splitlines(): # run tests commands
        try:
            prog.stdin.write(line + "\n")
            prog.stdin.flush()
            # switching the orders of the next three lines
            # so that exit test works.
            time.sleep(.2)
            if line.strip() == "exit" and prog.poll() == 0:
                break
        except IOError as e:
            if prog.returncode == None:
                to_print = "%s %s: the shell exited unexpectedly\nThe offending command was: %s\n" % \
                            (test_dir_to_str(os.path.basename(test_dir)), color_text("failed", "red"),
                            prev_line)
            else:
                to_print = "%s %s: error communicating with shell (%s)\n" % \
                            (test_dir_to_str(os.path.basename(test_dir)), color_text("failed", "red"),
                            str(e))
            if display_errors or verbose:
                if report_file:
                    report_file.write(to_print + "\n")
                else:
                    print(to_print)
            return None
        prev_line = line

    ret = ["", "", ""]
    class Alarm(Exception):
        pass
    def alarm_handler(signum, frame):
        raise Alarm
    signal.signal(signal.SIGALRM, alarm_handler)
    signal.alarm(6)
    try:
        ret = prog.communicate()
        signal.alarm(0)
    except Alarm:
        to_print = "%s %s: timed out\n" % \
                    (test_dir_to_str(os.path.basename(test_dir)), color_text("failed", "red"))
        if display_errors or verbose:
            if report_file:
                report_file.write(to_print + "\n")
            else:
                print(to_print)
        return None

    retOut, retErr = (ret[0].replace("\r", ""), ret[1].replace("\r", ""))
    return retOut.strip(), retErr.strip()


def readExpected(filepath: str, errMsg: str) -> str:
    """
    Read the expected output/error from the file at the  given path and returns its contents.
    Args:
        filepath (str): the path of the file to manipulate.
        errMsg (str): the error message to be printed out upon any IOError raised.
    Returns:
        str: the contents of the file
    """
    try:
        with open(filepath, "r") as ofile:
            output = ofile.read().strip()
            return output
    except IOError as e:
        print(errMsg)
        print(e)
        return None


def runTest(tests_dir: str, test_name: str, shell_path: str, 
            report_file: Optional[TextIOWrapper]) -> bool:
    """
    Runs the given test, printing out any error messages, and returning if it passed or not.
    Args:
        tests_dir (str): the absolute path to the overarching tests directory.
        test_name (str): the name of the test being run (the directory name).
        shell_path (str): the absolute path to the shell executable.
        report_file (Optional[TextIOWrapper]): the optional file to print a report to.
    Returns:
        (bool): whether the test passed or not.
    """
    test_dir = tests_dir + "/" + test_name
    chdir(playground)
    cleanDir(playground)
    setupDir(playground, test_dir)

    shellInput = readExpected(test_dir + "/" + testInput, "Error opening/reading input file:")
    if shellInput is None:
        sys.exit(1)
    # setupDir should create an expected output file in the directory
    goodOut = readExpected(playground + "/" + testOutput, "Error opening/reading expected output file:")
    if goodOut is None:
        sys.exit(1)
    goodErr = readExpected(test_dir + "/" + testError, "Error opening/reading expected error file:")
    if goodErr is None:
        sys.exit(1)

    student_ret = runShell(shell_path, test_dir, shellInput, report_file)
    if not student_ret:
        return False
    retOut, retErr = student_ret
    
    # make sure they all match
    out_failure = retOut != goodOut
    # err_failure = retErr != goodErr
    err_failure = len(retErr.splitlines()) != len(goodErr.splitlines())
    passed = not out_failure and not err_failure
    success_str = color_text("passed", "green") if passed else color_text("failed", "red")

    header = "%s %s" % (test_dir_to_str(test_name), success_str)
    if out_failure:
        header += ": stdout mismatch"
    if err_failure:
        header += ", stderr mismatch" if out_failure else ": stderr mismatch"

    def line_str(text: str) -> str:
        if text == "":
            return ""
        num_lines = len(text.splitlines())
        return f' ({num_lines} line{"s" if num_lines != 1 else ""})'

    if (display_errors and not passed) or verbose:
        if passed:
            to_print = header
        else:
            to_print = header + "\n"

            sections = [[("Input", shellInput)]]
            if out_failure:
                sections.append([("Expected stdout", goodOut), ("Received stdout", retOut)])
            if err_failure:
                sections.append([("Expected stderr", goodErr), ("Received stderr", retErr)])
            
            for subsection in sections:
                for subheader, text in subsection:
                    to_print += color_text(f"{subheader}:{line_str(text)}\n", attrs=["bold","underline"])
                    to_print += "----------------------\n"
                    to_print += text + "\n"
                    to_print += "----------------------\n"
                to_print += "\n"
        
        if report:
            report_file.write(to_print + "\n")
        else:
            print(to_print)
    return passed


def testStudent(spath: str, tests_to_run: "list[int]", report_path: Optional[str]) -> int:
    """
    Tests a student's shell implementation on the given tests.
    Args:
        spath (str): the absolute path to the shell implementation.
        tests_to_run (list[int]): the numbers of the tests to run.
        report_path (Optional[str]): the absolute path to the report file.
    Returns:
        int: the total score earned by the student.
    """
    os.chdir(playground)

    try:
        report_file = None
        if report:
            report_file = open(report_path, "w")

        results: dict[int, bool] = {}
        for test_num in tests_to_run:
            results[test_num] = runTest(testSuite, all_tests[test_num], spath, report_file)
        if verbose and results[tests_to_run[-1]]:
            if report:
                report_file.write("\n\n")
            else:
                print("\n")

        scores: dict[int, int] = {} # dictionary of max points possible per test
        for test_num in tests_to_run:
            with open(testSuite + "/" + all_tests[test_num] + "/" + testPoints, "r") as f:
                try:
                    scores[test_num] = int(f.read())
                except IOError:
                    print("Error: test point value undefined for test %d." % test_num)
                    scores[test_num] = 0
                except ValueError:
                    print("Error: invalid test point value for test %d" % test_num)
                    scores[test_num] = 0

        totScore = 0
        totPossible = 0
        if report_file:
            report_file.write("Report:\n")
            report_file.write("--------------------\n")
        else:
            print("Report:")
            print("--------------------")
            
        sorted_results: list[tuple[int, bool]] = list(results.items())
        sorted_results.sort(key=lambda item: item[0])

        PASS_STR = color_text("Passed", "green")
        FAIL_STR = color_text("Failed", "red")
        for test_num, passed in sorted_results:
            test_points = scores[test_num]
            test_name = test_dir_to_str(all_tests[test_num]) + ":"
            if passed:
                test_status = "%s (%d/%d)" % (PASS_STR, test_points, test_points)
                if report_file:
                    report_file.write(f'{test_name: <45} {test_status: >20}\n')
                else:
                    print(f'{test_name: <45} {test_status: >20}')
                totScore += scores[test_num]
            else:
                test_status = "%s (0/%d)" % (FAIL_STR, test_points)
                if report_file:
                    report_file.write(f'{test_name: <45} {test_status: >20}\n')
                else:
                    print(f'{test_name: <45} {test_status: >20}')
            totPossible += test_points

        if report_file:
            report_file.write("--------------------\n")
            report_file.write("Total: %d/%d\n" % (totScore, totPossible))
        else:
            print("--------------------")
            print("Total: %d/%d" % (totScore, totPossible))
            # TODO: remove this line if not using rubric from Fall 2019
            # print("Functionality Score [for TAs]: " + str(((float(totScore) / totPossible) * 60) // 1))
            print()

        if report_file:
            print("Report written to file!")
            report_file.close()

        return totScore
    except IOError as e:
        print("Unexpected I/O error: %s\nTesting failed." % str(e))
        return 0


def parseTests(tests: Optional[str]) -> "list[int]":
    """
    Parses the given list of tests, returning a sorted list of test numbers 
    from said test. For instance, "1-3,10,5-6" gives [1, 2, 3, 5, 6, 10]. If
    tests is None, returns a list of all tests.
    Args:
        tests (Optional[str]): an optional string describing the range of tests.
    Returns:
        list[int]: the range of tests, or exits on error.
    """

    if tests is None:
        test_nums = list(all_tests.keys())
    else:
        test_nums = []

        tokens = tests.split(",")
        try:
            for token in tokens:
                subtokens = token.split("-")
                if len(subtokens) == 1:
                    test_num = int(subtokens[0])
                    if test_num in all_tests:
                        test_nums.append(test_num)
                    else:
                        print("Invalid test number: %d" % test_num)
                        sys.exit(1)
                elif len(subtokens) == 2:
                    test_lower = int(subtokens[0])
                    test_upper = int(subtokens[1])
                    if test_lower >= test_upper:
                        print("Invalid range given: %d is not less than %d", test_lower, test_upper)
                        sys.exit(1)
                    
                    for test_num in range(test_lower, test_upper + 1):
                        if test_num in all_tests:
                            test_nums.append(test_num)
                        else:
                            print("Invalid test number: %d" % test_num)
                            sys.exit(1)
                else:
                    print("Invalid test selection string, use a string of the format: 1,3,5,7-12")
                    sys.exit(1)
        except ValueError:
            print("Invalid test selection string, use a string of the format: 1,3,5,7-12")
            sys.exit(1)
    test_nums.sort()
    return test_nums


def usage() -> None:
    print("Usage:", os.path.basename(__file__), "[-s <sh>] [-t <tests>] [-u <suite>] [-r <report file>] [-e] [-v] [-h] [-p]")

def help() -> None:
    print("Shell Autotester")
    print("----------------")
    print("\t[-t, --tests] <dir>: run just the given test numbers (e.g. 1-3,5,35-43)")
    print("\t[-u, --suite] <dir>: use the tests located in directory <dir>, defaults to ./shell_1_tests")
    print("\t[-s, --shell] <sh>: test the indicated shell executable <sh>, defaults to ./33noprompt")
    print("\t\tTo pass any tests, <sh> must print only program output.")
    print("\t[-r, --report] <file>: generate report in the file <file>")
    print("\t[-v, --verbose]: print each test to stdout as it is run.")
    print("\t[-e, --errors]: display failing tests as they are run.")
    print("\t[-p, --pseudoterm]: run tests in the pseudoterminal harness")
    print("\t[-h, --help]: display this message and exit.")


#########################
# Starting Main Body... #
#########################

tests = None
testSuite = "./shell_1_tests"
playground = tempfile.mkdtemp()
executable = "./33noprompt"
display_errors = False
verbose = False
pseudoterm = False
report = None

try:
    opts, args = gnu_getopt(sys.argv[1:], "t:u:s:r:pevh",
            ["tests=", "suite=", "shell=", "report=", "verbose", "errors", "pseudoterm", "help"])
except GetoptError as err:
    print(err)
    usage()
    sys.exit(2)
for opt, val in opts:
    if opt in ("-t", "--tests"):
        tests = val
    elif opt in ("-u", "--suite"):
        testSuite = val
    elif opt in ("-s", "--shell"):
        executable = val
    elif opt in ("-r", "--report"):
        report = val
    elif opt in ("-e", "--errors"):
        display_errors = True
    elif opt in ("-v", "--verbose"):
        verbose = True
    elif opt in ("-p", "--pseudoterm"):
        pseudoterm = True
    elif opt in ("-h", "--help"):
        help()
        sys.exit(0)
    else:
        print("Impossible option has been assigned.")
        usage()
        sys.exit(1)

# make sure all paths are absolute, for safety's sake.
# we'll be changing our cwd frequently, and don't want
# to lose track of the tests or handin.
executable = os.path.abspath(executable)
if report:
    report = os.path.abspath(report)

testSuite = os.path.abspath(testSuite)
playground = os.path.abspath(playground)

print("Testing %s\n" % executable)
# dictionary mapping each test number to its directory name
try:
    all_tests = {int(dir[:dir.find("_")]): dir for dir in os.listdir(testSuite + "/")}
except ValueError:
    print("Invalid naming of test directories - must start with <number>_")
    sys.exit(1)
tests_to_run = parseTests(tests)
testStudent(executable, tests_to_run, report)

cleanDir(playground)
